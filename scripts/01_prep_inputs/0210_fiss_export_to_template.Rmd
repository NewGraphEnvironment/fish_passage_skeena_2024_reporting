---
title: "0210_fiss_export_to_template"
date: "Created: 2024-06-20 | Updated: `r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    code_folding: "hide"
params:
  repo_owner: "NewGraphEnvironment"
  repo_name: "fish_passage_skeena_2024_reporting"
  gis_name: "sern_skeena_2023"
  job_name: "2024-072-sern-skeena-fish-passage"
editor_options: 
  chunk_output_type: console
---

# Purpose of this script

Export fiss site data to csv for cut and paste into the fish data submission spreadsheet (habitat_confirmations.xls). This script has been updated so that the csv contains as much of the data as possible, so that only a few copy/pastes are required. We are trying to move away from all the back and forths between the spreadsheet, R, and Q. 

```{r params}
path_form_fiss_site <- fs::path('~/Projects/gis/sern_skeena_2023/data_field/2024/form_fiss_site_2024.gpkg')
```



# Prepare the data for copy/paste into submission spreadsheet

After QAing the data in QGIS, now we will prepare it for copy/paste into submission spreadsheet.


## Backup the form after the QA

```{r backup}

form_fiss_site <- fpr::fpr_sp_gpkg_backup(
  path_gpkg = path_form_fiss_site,
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = FALSE,
  write_back_to_path = FALSE,
  return_object = TRUE,
  col_easting = "utm_easting",
  col_northing = "utm_northing"
) 
```


## Prepare the data for copy paste to spreadsheet.

```{r cp-prep}
# see the names of our form
names(form_fiss_site_prep2)

# let's get the names of the input template
# there is lots of work to do to pull out all the information we can use so we will start with one small step at a time
# lets just populate the location and site info pages.
form_raw_names_site <- fpr::fpr_import_hab_con(backup = F,
  row_empty_remove = T) |> 
  # pull out just the site info page for now
  pluck(4) |> 
  # only keep the names of the columns
  names()

# location names
form_raw_names_location <- fpr::fpr_import_hab_con(backup = F,
  row_empty_remove = T) |> 
  # pull out just the site info page for now
  pluck(1) |> 
  # only keep the names of the columns
  names()



# we don't want duplicate column names because it messes with them (renames them both) so we need to get rid of dupes
names_dup <- intersect(form_raw_names_site, form_raw_names_location)

# join the names of our two target tables together without (!) the dupes
form_raw_names_sl <- c(form_raw_names_location,
                       form_raw_names_site[!form_raw_names_site %in% names_dup])


# Rename the columns to match those in the spreadsheet
form_site_info_prep <- form_fiss_site_prep2 |> 
  sf::st_drop_geometry() |> 
  # alias local name and gazetted_name is not called the same in both sheets so rename
  dplyr::rename(alias_local_name = local_name,
                gazetted_name = gazetted_names,
                waterbody_id_identifier = waterbody_id,
                watershed_code_45_digit = watershed_code_50k) |> 
  dplyr::mutate(utm_method = "GPS General",
                reference_number = dplyr::row_number()) |> 
  dplyr::select(mergin_user,
                date_time_start,
                dplyr::contains('surveyor'),
                dplyr::any_of(form_raw_names_sl))

```


## Filter and c/p the reference and location data

Filter out the location data and burn to csv for copy paste into `step_1_ref_and_loc_info`

```{r cp-ref-location}
# make the reference and location form to c.p into `step_1_ref_and_loc_info`
form_fiss_loc <- bind_rows(

  # we need the raw form or we don't have all the right columns
  fpr::fpr_import_hab_con(backup = F,
                          row_empty_remove = T) |> 
    # pull out just the site info page for now
    purrr::pluck("step_1_ref_and_loc_info") |> 
    dplyr::slice(0) |> 
    dplyr::mutate(dplyr::across(dplyr::everything(), as.character)),
      
   form_site_info_prep |> 
    dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |> 
    dplyr::select(dplyr::any_of(form_raw_names_location))) |> 
  
  dplyr::mutate(site_number = dplyr::row_number())


# burn to file
form_fiss_loc |> 
  readr::write_csv(paste0(
    'data/inputs_extracted/form_fiss_loc_tidy',
    '.csv'),
    na = '')

# Now copy paste the date from the csv into `step_1_ref_and_loc_info`
```


## Filter and c/p stream site data

Filter out the stream site data and burn to csv for copy paste into `step_4_stream_site_data`

```{r cp-stream-site}
# make the stream site form to c/p into `step_4_stream_site_data`
form_fiss_site <- bind_rows(

  # we need the raw form or we don't have all the right columns
  fpr::fpr_import_hab_con(backup = F,
    row_empty_remove = T) |> 
    # pull out just the site info page for now
    purrr::pluck("step_4_stream_site_data") |> 
    dplyr::slice(0) |> 
    dplyr:: mutate(dplyr::across(dplyr::everything(), as.character)),
                   
  form_site_info_prep |> 
    dplyr:: mutate(dplyr::across(dplyr::everything(), as.character)) |>
    # these column names differ slightly from those in `step_1_ref_and_loc_info`, so we update them so they are included in the csv for easier c/p.
    dplyr::rename(local_name = alias_local_name,
                gazetted_names = gazetted_name,
                waterbody_id = waterbody_id_identifier) |> 
    select(dplyr::any_of(form_raw_names_site),
           # add the time to help put the puzzle together after)
           survey_date)
) |> 
  select(everything())


# burn to file
form_fiss_site |> 
  readr::write_csv(paste0(
    'data/inputs_extracted/form_fiss_site_tidy',
    '.csv'),
    na = '')


# Now copy paste the date from the csv into `step_4_stream_site_data`

```

Next populate step 3 in fish_data_tidy.R then populate step 2 in 0100-extract-inputs.R.


