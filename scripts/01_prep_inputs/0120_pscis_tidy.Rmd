---
title: "0120-pscis-tidy"
date: "Created: 2024-06-20 | Updated: `r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    code_folding: "hide"
params:
  repo_owner: "NewGraphEnvironment"
  repo_name: "fish_passage_skeena_2023_reporting"
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=TRUE, include = TRUE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = "100%", eval = FALSE)
options(scipen=999)
options(knitr.kable.NA = '--') #'--'
options(knitr.kable.NAN = '--')
```

# Purpose of this script

This scripts is used to prepare data for QA in QGIS. 

This script is currently being used to clean the 2024 Peace `form_pscis_2024.gpkg` file.

First we load the necessary packages.
```{r}
source('scripts/packages.R')
```



## Fetch and backup the data

Name the project directory we are pulling from and import the data. We use `fpr_sp_gpkg_backup` to import and backup the data, as well as update the UTMS. We will also write the data to a csv and Rdata file and commit for version control using git.

```{r import}
path <- "~/Projects/gis/sern_peace_fwcp_2023"

form_pscis <- fpr_sp_gpkg_backup(
  path_gpkg = paste0(path, "/data_field/2024/form_pscis_2024.gpkg"),
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = F,
  write_back_to_path = FALSE,
  write_to_csv = TRUE,
  write_to_rdata = TRUE,
  return_object = TRUE)
  
```


## Clean and QA the data

First we check for duplicated sites. No duplicates were found.

```{r dups}
form_pscis |> 
  filter(!is.na(my_crossing_reference)) |>
  group_by(my_crossing_reference) |>
  filter(n()>1)
```


We need to check for sites that have a culvert length (length_or_width_meters) over 99.9 or a fill depth (fill_depth_meters) over 9.9, anything over this will cause errors in submission sheet. If over the maximums, we will change them to 99.9 and 9.9, respectively, and will append a note to the assessment comments.


```{r max-length-fill}

# First lets check
form_pscis |> 
  filter(length_or_width_meters > 99.9 | fill_depth_meters > 9.9)

## No sites were found. Sweet, move on.

# Now we will change them to 99.9 and 9.9, respectively, and will append a note to the assessment comments.
form_pscis <- form_pscis |>
  dplyr::mutate(
    assessment_comment = case_when(
      length_or_width_meters > 99.9 ~ paste0(assessment_comment, 'Culvert length ', length_or_width_meters, 'm but changed to 99.9m to meet submission requirements.'),
      TRUE ~ assessment_comment
    ),
    length_or_width_meters = case_when(
      length_or_width_meters > 99.9 ~ 99.9,
      TRUE ~ length_or_width_meters
    ),
    assessment_comment = case_when(
      fill_depth_meters > 9.9 ~ paste0(assessment_comment, 'Fill depth ', fill_depth_meters, 'm but changed to 9.9m to meet submission requirements.'),
      TRUE ~ assessment_comment
    ),
    fill_depth_meters = case_when(
      fill_depth_meters > 9.9 ~ 9.9,
      TRUE ~ fill_depth_meters
    )
  )

```


We need to fix the times because they are in UTC and we need them in PDT. This issue is documented here https://github.com/NewGraphEnvironment/fish_passage_template_reporting/issues/18

For peace 2024, the times are correct in form_pscis but are incorrect (in UTC) when form_pscis_2024 gets read in in Q. Will deal with this in the next step when the QA'd data gets processed. The following script was not run for Peace 2024.


```{r time-fix}

form_pscis <- form_pscis |> 
# make a new column for the time as is with different name then mutate to PST
  # we don't need the new column but will leave here for now so we can visualize and confirm the time is correct
  mutate(date_time_start_raw = date_time_start,
         date_time_start = lubridate::force_tz(date_time_start_raw, tzone = "America/Vancouver"),
         date_time_start = lubridate::with_tz(date_time_start, tzone = "UTC")) |> 
  relocate(date_time_start_raw, .after = date_time_start)

## Double check the time is correct and now remove the date_time_start_raw column
form_pscis <- form_pscis |> 
  select(-date_time_start_raw)
```



Now we will:
- remove the site used to make the form
- split date time column into date and time
- Fix some vocabulary
- back up the original assessment comments so we can redo this amalgamation of text if we need to
- reorder the columns for easy QA in Q

```{r clean-up }
form_pscis_cleaned <- form_pscis |>
  # remove the site used to make the form
  filter(site_id != '12345' | !is.na(date_time_start)) |>

  #split date time column into date and time
  dplyr::mutate(
    date_time_start = lubridate::ymd_hms(date_time_start, tz = "America/Vancouver"),
    date = lubridate::date(date_time_start),
    time = hms::as_hms(date_time_start),
    
  # Fix the vocabulary
    stream_name = str_replace_all(stream_name, 'Trib ', 'Tributary '),
    road_name = str_replace_all(road_name, 'Hwy ', 'Highway '),
    crew_members = toupper(crew_members),
  
  # back up the original assessment comments so we can redo this amalgamation of text if we need to
    assessment_comment_og = assessment_comment,
  ) |>
  select(-time) |>

  # we want these new columns to land at a logical place in the table so we will reorder them
  dplyr::select(
    date_time_start,
    crew_members,
    pscis_crossing_id,
    my_crossing_reference,
    my_priority,
    assessment_comment,
    contains("_notes"),
    contains("moti_chris_culvert_id"),
    contains("my_citation_key"),
    everything()) %>%
  arrange(crew_members, date_time_start)
```

## Backup and burn back to geopackage

Now we burn the cleaned copy to QGIS project as new file in the `/data_field/2024/form_pscis_2024.gpkg` format. Finally, we backup the cleaned data to a csv and Rdata file and commit for version control using git.

```{r backup}
# burn the cleaned copy to QGIS project as new file
form_pscis_cleaned %>%
  sf::st_write(paste0(path, '/data_field/2024/form_pscis_2024.gpkg'), append=F, delete_dsn=T)

#backup the cleaned data to a csv and Rdata and commit for version control using git
fpr_sp_gpkg_backup(
  path_gpkg = paste0(path, '/data_field/2024/form_pscis_2024.gpkg'),
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = TRUE,
  write_back_to_path = FALSE,
  write_to_csv = TRUE,
  write_to_rdata = TRUE,
  return_object = FALSE)
```

