---
title: "0200-fiss-site-tidy"
date: "Created: 2024-06-20 | Updated: `r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    code_folding: "hide"
params:
  repo_owner: "NewGraphEnvironment"
  repo_name: "fish_passage_template_reporting"
  gis_name: "sern_peace_fwcp_2023"
  job_name: "2024-073-sern-peace-fish-passage"
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=TRUE, include = TRUE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = "100%", eval = FALSE)
options(scipen=999)
options(knitr.kable.NA = '--') #'--'
options(knitr.kable.NAN = '--')
```


# Purpose of this script

This scripts is used to prepare fiss site data (phase 2) for submission to the province. 

This script is currently being used to clean the 2024 Peace `form_fiss_site_2024.gpkg` file.


# Clean the data in preparation for QA in QGIS 

## Set up the parameters

To set up the parameter, update the `gis_name` and the `repo_name` in the YAML at the beginning of this script. 
Then update the year in the code below.

```{r params}
year = "2024"
file = "form_fiss_site_2024.gpkg"

path_form_fiss_site <- fs::path_expand(fs::path("~/Projects/gis/", params$gis_name, "/data_field/", year, file))
```

## Fetch and backup the data

The first step is to remove by-hand the first row in `form_fiss_site_2024.gpkg` because it is just a place holder. It has `stream_name` set to `Robert Hatch`

Then we use `fpr_sp_gpkg_backup` to import and backup the data, as well as update the UTMS. We will also write the data to a csv and Rdata file and commit for version control using git.

```{r import}

form_fiss_site_raw <- fpr::fpr_sp_gpkg_backup(
  path_gpkg = path_form_fiss_site,
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = FALSE,
  write_back_to_path = FALSE,
  return_object = TRUE,
  col_easting = "utm_easting",
  col_northing = "utm_northing"
) 
```


## Identify duplicate sites

Identify duplicate sites (that are not NAs) as we don't want to input two of the same. 
If there are duplicates go back to QGIS and clean up in `form_fiss_site_2024`. Then read import using scripts above. 

Peace 2024 - No duplicate sites found

```{r duplicates}
dups <- form_fiss_site_raw  |> 
  filter(!is.na(local_name)) |> 
  group_by(local_name) |> 
  filter(n()>1)
```


## Clean up the form 

```{r clean-form}
# clean up the form
form_fiss_site_cleaned <- form_fiss_site_raw |> 
  # split the local_name into the site_id and the location
  tidyr::separate(local_name, into = c('site_id', 'location'), remove = F, extra = "merge") |> 
  # split out the date and the time - change type of column first
  dplyr::mutate(date_time_start = lubridate::ymd_hms(date_time_start),
                date_time_start = lubridate::floor_date(date_time_start, unit = "second"),  # Remove microseconds
                survey_date = lubridate::date(date_time_start),
                time = hms::as_hms(date_time_start), 
                # Fix some vocabulary. Change "trib" to long version "Tributary" etc.
                gazetted_names = str_replace_all(gazetted_names, 'Trib ', 'Tributary '),
                crew_members = toupper(crew_members),
                # fill in text columns from spreadsheet that will likely never change
                waterbody_type = 'stream',
                method_for_utm = 'GPS general',
                method_for_channel_width = 'metre tape',
                method_for_wetted_width = 'metre tape',
                method_for_residual_pool_depth = 'metre stick',
                method_for_bankfull_depth = 'metre stick',
                method_for_gradient = 'clinometer',
                method_for_temperature = 'recording meter',
                method_for_conductivity = 'recording meter',
                method_for_p_h = 'pH meter (general)') |> 
  # arrange by surveyor and date/time
  dplyr::arrange(mergin_user, date_time_start) |> 
  dplyr::mutate(comments = paste0(comments, time)) |> 
  # ditch the time since we don't need anymore. Time was dropped on gpkg creation due to type conflict
  select(-time) |>
  # rearrange the columns for easier QA in QGIS.
  dplyr::select(
    date_time_start,
    local_name,
    gazetted_names,
    crew_members,
    comments,
    everything()) |> 
  arrange(date_time_start)

```


## Burn a cleaned copy to the QGIS project for QA

```{r burn-form-fiss-site}
# Burn cleaned copy to the QGIS project
form_fiss_site_cleaned |> 
  sf::st_write(path_form_fiss_site, append=FALSE, delete_dsn = T)
```

Now open QGIS and re-add the `form_fiss_site_2024` layer. Check to see the form is updated with the changes made here. Copy/paste all
`styles` from the old layer to the new layer you just added.  Lastly, you can delete the old layer and start QAing `form_fiss_site_2024.` 



# Prepare the data for copy/paste into submission spreadsheet

After QAing the data in QGIS, now we will prepare it for copy/paste into submission spreadsheet.

```{r}

# -----------------read in form after review and finalization in QGIS------------------
# read in the form
form_fiss_site_raw <- sf::st_read(dsn= paste0('../../gis/', dir_gis, '/data_field/2023/form_fiss_site_2023.gpkg')) |> 
  # need to convert date type b/c gpkg and excel import differently
  mutate(survey_date = lubridate::as_date(survey_date))

# Time issue-----------------------------------------------------------------------------------------------------
# Skeena 2023 fiss form has the incorrect time and needs to have 7hrs added to be in PDT. This is a temporary fix.
# First double check the repaired time is correct
fiss_correct_time_check <- form_fiss_site_raw |>
  # there is perhaps a more reliable method for this issue in
  # Seems like  more reliable method way to do would be to use force_tz and with_tz since it takes into account daylight savings time
  # https://github.com/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/49
  # I still can't get this method to work, so sticking with what's below for now
  dplyr::mutate(date_time_start_repaired = date_time_start + lubridate::hours(7)) |>
  dplyr::relocate(date_time_start_repaired, .after = date_time_start)

# If correct -  replace - we will replace object to avoid confusion in future when we hopefully don't need this step
form_fiss_site_raw <- fiss_correct_time_check |>
  dplyr::mutate(date_time_start = date_time_start_repaired) |>
  dplyr::select(-date_time_start_repaired)

# see the names of our form
names(form_fiss_site_raw)

# let's get the names of the input template
# there is lots of work to do to pull out all the information we can use so we will start with one small step at a time
# lets just populate the location and site info pages.
form_raw_names_site <- fpr::fpr_import_hab_con(backup = F,
  row_empty_remove = T) |> 
  # pull out just the site info page for now
  pluck(4) |> 
  # only keep the names of the columns
  names()

# location names
form_raw_names_location <- fpr::fpr_import_hab_con(backup = F,
  row_empty_remove = T) |> 
  # pull out just the site info page for now
  pluck(1) |> 
  # only keep the names of the columns
  names()



# we don't want duplicate column names because it messes with them (renames them both) so we need to get rid of dupes
names_dup <- intersect(form_raw_names_site, form_raw_names_location)

# join the names of our two target tables together without (!) the dupes
form_raw_names_sl <- c(form_raw_names_location,
                       form_raw_names_site[!form_raw_names_site %in% names_dup])


# tidy our populated table to PASTE SPECIAL value only!!! to our template. Might need to be in chunks but so be it
form_site_info_prep <- form_fiss_site_raw |> 
  dplyr::select(rowid,
                mergin_user,
                date_time_start,
                dplyr::contains('surveyor'),
                dplyr::any_of(form_raw_names_sl))

# make the loc form
form_fiss_loc <- bind_rows(

  # we need the raw form or we don't have all the right columns
  fpr::fpr_import_hab_con(backup = F,
                          row_empty_remove = T) |> 
    # pull out just the site info page for now
    pluck("step_1_ref_and_loc_info") |> 
    mutate(survey_date = lubridate::as_date(survey_date)) |> 
    slice(0) |> 
    mutate(dewatered_dry_int_channel = as.character(dewatered_dry_int_channel)),

  form_site_info_prep |> 
    sf::st_drop_geometry() |> 
    # alias local name and gazetted_name is not called the same in both sheets so rename
    rename(alias_local_name = local_name,
           gazetted_name = gazetted_names) |> 
    mutate(utm_method = as.character(utm_method)) |> 
    select(rowid, dplyr::any_of(form_raw_names_location))) |> 
  mutate(site_number = dplyr::row_number(),
         reference_number = dplyr::row_number())



# make the site form
form_fiss_site <- bind_rows(

  # we need the raw form or we don't have all the right columns
  a <- fpr::fpr_import_hab_con(backup = F,
    row_empty_remove = T) |> 
    # pull out just the site info page for now
    pluck("step_4_stream_site_data") |> 
    slice(0) |> 
    mutate(feature_height_length_method = as.character(feature_height_length_method),
           utm_method = as.character(utm_method)),

  form_site_info_prep |> 
    sf::st_drop_geometry() |> 
    mutate(morphology = as.character(morphology),
           utm_method = as.character(utm_method)) |> 
    select(rowid,
           dplyr::any_of(form_raw_names_site),
           # add the time to help put the puzzle together after)
           survey_date)
) |> 
  select(rowid, everything())

# burn to file
# The following fields need to be added by hand in the spreadsheet:
# UTM method, No Visible Channel, Waterbody (ID) Identifier (this should be scripted eventually)
form_fiss_loc |> 
  readr::write_csv(paste0(
    'data/inputs_extracted/form_fiss_loc_tidy',
    '.csv'),
    na = '')


form_fiss_site |> 
  readr::write_csv(paste0(
    'data/inputs_extracted/form_fiss_site_tidy',
    '.csv'),
    na = '')

## Next populate step 3 in fish_data_tidy.R then populate step 2 in 0100-extract-inputs.R.

```



