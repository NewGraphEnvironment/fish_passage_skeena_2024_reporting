# FDS cleanup and finalization
Used for 2024 data submission

First we need to fill in everything in the Step 1 sheet. 

Project Title: Same as report - "Restoring Fish Passage in the Skeena Region - 2024"
Company/Agency: Other
Company/Agency (Other): New Graph Environment Ltd.
Spreadsheet Recorder(s): Allan Irvine or Lucy Schick
Project Type: Research
PROVINCIAL PERMIT NUMBER: `permit_id` 


The information in this submission has been reviewed and verified by a Registered Professional Biologist (Pick list: Yes or No)					Yes	
Biologist's Name:					Allan Irvine	
Registration Number:					2775	
Province of Registration:					British Columbia	

## Define paths and params
Some of this could move to params in the future but for now we will just define them here.

```{r}
permit_id <- 'PG24-879256'

project_region <- stringr::str_match(params$repo_name, "fish_passage_([^_]+)_")[, 2]

stub_onedrive_submission <- fs::path_expand(fs::path("~/Library/CloudStorage/OneDrive-Personal/Projects/submissions/fish_data", params$project_year, project_region))

path_in <- fs::path("data/habitat_confirmations.xls")

stub_out_csv <-  fs::path("data/inputs_extracted")

stub_out_permit <- fs::path("data/permit_submission")


fs::dir_create(stub_out_permit)

```



## Duplicate the FDS and make final changes for submission
Now we will make a copy of the data. We don't love having multiple values of the same data but we don't want to break any of
the past workflows.

```{r}
fs::file_copy(path = path_in,
          new_path = fs::path(stub_out_permit, paste0(permit_id, "_data.xls")),
          overwrite = TRUE)

```

<br>

Now we reorganize our spreadsheet so that we remove `step_4_stream_site_data` when our sites are just small electrofishing 
sites.  We also want there to link our `alias_local_name` to the user so they can link our reporting to provincial datasets.
How we will do this by sheet in the workbook:


- `step_2_fish_coll_data`
  + Append comments from `step_4_stream_site_data` to the comments column.

- `step_4_stream_site_data`
  + remove all `*_ef*` sites. This will require erasing everything and then pasting back in the data from the dataframe.
  

### `step_4_stream_site_data` - Copy the `alias_local_name` to `comments` 

**We now do all work on the `data/permit_submission/{permit_number}_data.xls` file.**

<br>

Apparently it does not matter if we leave our unique id in this column but it is not going to land in provincial datasets
if we don't put it somewhere else. We want people to be able to cross reference data in the provincial systems to our reports so we add the id to the comments of `step_4_stream_site_data` sheet.  

```{r copy-alias-step4}

hab_con <- fpr::fpr_import_hab_con(backup = F, row_empty_remove = T, col_filter_na = T)

# now make a csv with the alias_local_name pasted to the comments so people can see which comments are linked to which site in the reports
site_comments <- hab_con |> 
  purrr::pluck("step_4_stream_site_data") |> 
  dplyr::mutate(comments = paste0('Site ', local_name, '. ', comments)) |> 
  dplyr::select(reference_number, gazetted_names, local_name, comments)

site_comments |> 
  readr::write_csv(fs::path(stub_out_csv, "hab_con_alias_local_name_relocate.csv"))
```

<br>

Since we redo `step_4_stream_site_data` anyways to remove the ef sites (at the end), we should just add the updated comments then.


### `step_2_fish_coll_data` - add all comments (now with site ids included) to the comments 
Because one of our next steps is to remove the `_ef*` sites from `step_4_stream_site_data`, `step_2_fish_coll_data` will now be the
only place where our comments now live. It may not end up accessible in the data layers we pull from BC Data 
Catalogue but we do it in case it is helpful.


<br>

First we run a test to see if there is an `ef` in the `alias_local_name` column AND
if there is an associated site that has the same "12345_us" but does not have the "_ef*" in the `alias_local_name`.
First step is just to answer the question of which sites have a non-ef site associated with them. We do not use this
information in the final product but it is maybe useful to know.

```{r test-reference-update}

# separate the alias_local_name
site_match_prep <-   hab_con |> 
    purrr::pluck("step_1_ref_and_loc_info") |> 
    dplyr::select(reference_number, alias_local_name) |> 
    tidyr::separate(alias_local_name, into = c('site', 'location', 'ef'), remove = F) |> 
  #now make a column that has the site and location put back together as site_id
  dplyr::mutate(site_id = paste(site, location, sep = "_")) 


# these sites have no ef
site_match_na_ef <- site_match_prep |> 
  dplyr::filter(is.na(ef)) 

# these sites have ef
site_match_ef <- site_match_prep |> 
  dplyr::filter(!is.na(ef)) |> 
  dplyr::pull(site_id) 

# filter out the site_match_na_ef that have a corresponding ef site
site_match_na_ef_yes <- site_match_na_ef |> 
  dplyr::filter(site_id %in% site_match_ef) 

```


```{r copy-comments-step2}

#so we update the method number only when the site has an ef site associated with it
site_comments_prep <- site_comments |> 
    dplyr::select(reference_number, comments) |>
  #tidyr::replace_na with "" so we don't get NAs
  tidyr::replace_na(list(comments = ""))

step_2_prep <- hab_con |> 
  purrr::pluck("step_2_fish_coll_data") |> 
  # change comments from logical to character so replace_na works 
  dplyr::mutate(comments_step2 = as.character(comments)) |> 
  dplyr::select(reference_number, 
                local_name, 
                method_number, 
                comments_step2) |> 
  tidyr::replace_na(list(comments_step2 = ""))


site_comments_step2 <- dplyr::left_join(
  
    step_2_prep,
    
    site_comments_prep,
    
    by = 'reference_number') |> 
  dplyr::mutate(comments_step2 = dplyr::case_when(!is.na(comments_step2) ~ paste(comments, comments_step2, sep = " "),
                                                  TRUE ~ comments)) |> 
  dplyr::select(reference_number, local_name, method_number, comments_step2)

# burn out our csv with the updated comments (and method number and utms if necessary)
site_comments_step2 |> 
    readr::write_csv(fs::path(stub_out_csv, "hab_con_method_comments_step2.csv"))

```

<br>

Ok - now we do the hand work to transfer `data/inputs_extracted/hab_con_method_comments_step2.csv` to `step_2_fish_coll_data` of the `data/permit_submission/{permit_number}_data.xls` file.

 
### Remove `step_4_stream_site_data` sites that are `_ef*` sites 
We have lots of sites that are return visits where we are just sampling and not doing 100m habitat sites.
For these we will leave all the locational information in the `step_1_ref_and_loc_info` sheet and remove all the `step_4_stream_site_data`.
This way we will at least not clutter the database with information that is not required or desired. We will leave the 
`step_3_individual_fish_data` sheet even though it is not actually required because it seems useful.


!!NOTE!! - code in above chunk should be used.  Can be moved to below.  can add new comments as same time as removeing ef sites but want to have procedures to remove NA when present and case_when for eliminating unesessary white space. 

```{r remove-ef-sites}

# now we remove the ef sites from the step_4_stream_site_data sheet
step_4_prep <- hab_con |> 
  purrr::pluck("step_4_stream_site_data") |> 
  dplyr::filter(!grepl("ef", local_name)) |> 
  #remove the old comments
  dplyr::select(-comments) |> 
  #add the new comments
  dplyr::left_join(site_comments_prep, 
             by = 'reference_number')

# burn out our csv so we can hand-bomb
step_4_prep |> 
  readr::write_csv(fs::path(stub_out_csv, "hab_con_step4_no_ef.csv"), na = "")


```

<br>

Removed data for all sites and copy pasted special back in the data from the `data/inputs_extracted/hab_con_step4_no_ef.csv` 
to `step_4_stream_site_data` sheet of `data/permit_submission/{permit_number}_data.xls`.  Need to pull up the formulas for averages and deal with `ph` significant digits but other than that it seemed to work fine.


<br>

Be sure to run `fpr::fpr_import_hab_con(row_empty_remove = T)` so we have record of changes viewable on github.

<br>

File "should" be QA'd with the submission QA tool.  This however requires windows. For Peace will copy to 
onedrive. Transfer to local windows machine. Run QA, record issues and then copy back over. This will cause
divergence with `habitat_confirmations.xls` file that should perhaps be remedied.

<br>

Copy data to onedrive to get to windows machine.

```{r}
#make a new directory for this years data submission
fs::dir_create(fs::path(stub_onedrive_submission))


fs::file_copy(path = fs::path(stub_out_permit, paste0(permit_id, "_data.xls")),
              new_path = fs::path(stub_onedrive_submission, paste0(permit_id, "_data.xls")),
              overwrite = TRUE)

```

Run the QA tool.  Record issues with github and fix in repo permit copy. 


**Still to do**

Fix in habitat_confirmations.xls - Rerun `fpr::fpr_import_hab_con(row_empty_remove = T)` and push to github.

```{r}
fpr::fpr_import_hab_con(row_empty_remove = T, col_filter_na = T)
```




