---
title: "120-photos"
date: "Created: 2024-04-26 | Updated: `r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    code_folding: "hide"
params:
  repo_owner: "NewGraphEnvironment"
  repo_name: "fish_passage_skeena_2023_reporting"
---

```{r setup, echo=TRUE, include = TRUE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = "100%", eval = FALSE)
options(scipen=999)
options(knitr.kable.NA = '--') #'--'
options(knitr.kable.NAN = '--')
```

# Notes about this particular script

1.  Chunk `eval` is set to `FALSE` as the default as much of what was run cannot be replicated due the resizing of photos and moveing of photos from one place to the other. If we want something rendered we need to explicity turn those chunks on.
2.  Photo storage has been the most complicated of all our data storage procedures to the number, size, multiple initial storage locations as well as the processing. We have started some documentation regarding that many places but are trying to figure out how to consolidate [here](https://github.com/NewGraphEnvironment/mybookdown-template/issues/26).
3.  This script was originally run with `form_pscis.gpkg` and `sf::st_read` in main directory of our `QGIS` project but altered after the fact to be up to date with our new workflows. Updated script was run as is from the QA section down.
4.  script was run on a mac and the original photos were stored on a local drive and backed up to `Digital Ocean`.\
5.  Although we usually try to abstract the location of our home directories (ex. swap out `Users/airvine` for `~/`) this can create hiccups for some of our functions that copy files from one place to another. It is likely possible to swap out `~/` for the expanded path using `path.expand(~/)` as demonstrated below but this was not done at the time this script was originally run.
6.  Just found a setting in the Rmarkdown global options that deals with `.Rmd` files reading from their working directory vs the project working directory. Info is here <https://bookdown.org/yihui/rmarkdown-cookbook/working-directory.html>. Wish I found this a looong time ago!!!!

```{r path-expand, eval = TRUE}
path.expand('~/')


```

Update 2024-09-21.  `here` is a great package and so is `fs`.  They help us deal with paths and rendering locations.


# Define Variables and Import Field Data

-   our shared photos location (this is where they live when they are all sorted and resized)
-   mergin project directory
-   input and output files for this script

<br>

```{r def-var, eval = TRUE}
dir_photos_shared = "~/Library/CloudStorage/OneDrive-Personal/Projects/2024-073-sern-peace-fish-passage/data/photos"
dir_project <- '~/Projects/gis/sern_peace_fwcp_2023'
file_in <- 'scripts/01_prep_inputs/0120-photos.Rmd'
# This assumes we use the same directory for the output file as the input file
file_out <- '0120-photos.html'
```

<br>

Now we import our field data. We turn on the `update_site_id`, `write_to_csv` and `update_utm` flags just in case sites were moved in QGIS or the data was otherwise updated. We also sort the data by `site_id` for easier reference. **Note** - Because we are running `fpr_sp_gpkg_backup` inside of a `Rmd` file we need to spell out the `dir_backup` argument as our working directory is different here when our file is not kept in the main project directory.

```{r import, eval = TRUE}
form_pscis <- fpr::fpr_sp_gpkg_backup(
  path_gpkg = fs::path(dir_project, 'form_pscis.gpkg'),
  dir_backup = fs::path(here::here(), "data", "backup"),
  update_site_id = TRUE,
  update_utm = TRUE,
  write_to_csv = TRUE,
  write_to_rdata = FALSE,
  return_object = TRUE
) |>
  dplyr::arrange(site_id)

```

# Make photo directories

Create the data and photos folder in shared file location. Onedrive for the time being.

```{r dir-create1}
dir.create(dir_photos_shared, recursive = T)
```

We do a bit of QA on our data.

-   check for duplicate sites
-   check for empty sites

```{r qa1, eval = TRUE}
# check for duplicate sites
form_pscis %>%
  dplyr::filter(!is.na(site_id)) %>%
  group_by(site_id) %>%
  dplyr::filter(n()>1) %>%
  nrow()

# check for empty sites
form_pscis %>%
  dplyr::filter(is.na(site_id)) %>%
  nrow()
```

<br>

Then we create the directories for the photos. We use the `fpr::fpr_photo_folders` function to create the directories passing it a vector of site_ids. We use the `purrr::map` function to iterate over the vector of site_ids.

```{r dir-create2}
form_pscis %>%
  pull(site_id) %>%
  as.character() %>%
  purrr::map(fpr::fpr_photo_folders, path = dir_photos_shared)
```

# Resize Photos

**NOTE** - there is a missing step here where the photos were transferred from a local drive to `onedrive` and resized. Not a manual step but documentation apears to be lacking from when it was done. We have been storing original photos locally on Al's mac and backing them up to `Digital Ocean`. We don't store them on onedrive because they are so large (each iphone 11 photo can be over 5mb!). We have only stored resized versions of onedrive....

<br>

Here we resize photos stored in our QGIS project which got there through upload through `Mergin` via our digital field forms created [here](https://github.com/NewGraphEnvironment/dff-2022/tree/master/scripts).

<br>

We also always change the extension to JPG for consistency and unforeseen issues (seen some before...). In addition we track major changes that happen in our QGIS projects through github reporting repositories using the `Mergin Tracking` issue. Major changes to `QGIS` are synced to Mergin and there is an associated `Mergin` version number. We record that and the details of what happened in the issue. You can view the "Mergin Tracking" issue for this repo [here](https://github.com/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/60).

<br>

There is an extra little move here. `photos.txt` is a "dummy file" that is created during QGIS project creation so the `ignore_mobile/photos/` directory is recognized as existing by `Mergin` which seems to use `git` under the hood to track changes (`git` cannot see empty directories either). We remove it here because it complicates our workflows that read all the names of each file within the directories that we ask it to detail.

<br>

Let's remove `r paste0(dir_project, "/ignore_mobile/photos/photos.txt")`

```{r rm-txt}

file.remove(paste0(dir_project, "/ignore_mobile/photos/photos.txt"))

```

<br>

Resize the photos and keep them on the server for now for collaboration with folks not on onedrive (ex. Gitskan Watershed Authorities).

```{r resize-mergin}

fpr_photo_resize_batch(
  dir_source = '/Users/airvine/Projects/gis/sern_skeena_2023/ignore_mobile/photos/',
  dir_target = '/Users/airvine/Projects/gis/sern_skeena_2023/ignore_mobile/photos_resized/')

# could erase photos here but will do by hand for safety
# recreate the photos.txt file so the form still works
file.create('/Users/airvine/Projects/gis/sern_skeena_2023/ignore_mobile/photos/photos.txt')

# back the photos up to onedrive (should remove all from mergin)
fpr::fpr_photo_resize_batch(
  dir_source = '/Users/airvine/Projects/gis/sern_skeena_2023/ignore_mobile/photos/photos_resized/',
  dir_target = '/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/mergin/')
```

When the above script was done - I deleted all the orignal `ignore_mobile/photos/` and added back the `photos.txt` so the fieldform is still functional - that should be scripted next time this is done.

<br>

# Rename the photos from the FISS cards and remove duplicates

When a photo is renamed using `fpr::fpr_photo_rename` the photo renamed is not duplicated. However, we take many more photos on our phones than we upload to `Mergin` via our field forms. We subsequently transfer all of our field photos off of our phones onto company drives. Because we use the gallery to upload our photos to `Mergin` (an important procedure so we don't lose photos when `mergin` glitches) we have a lot of duplicates. We use the `fpr::fpr_photo_remove_dupes` function to remove duplicates. We also have a `min_replicates` argument that allows us to remove photos that are not duplicated at least `n` times.

```         
?fpr::fpr_photo_rename
?fpr::fpr_photo_remove_dupes
```

```{r rename}
fpr::fpr_photo_rename(
  dat = form_fiss_site_raw,
  dir_from_stub = '/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/mergin/',
  dir_to_stub = '/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/',
  col_string_add = TRUE)

# used fpr_photo_remove_dupes to get rid of the first dupes
photos_dry_run <- fpr::fpr_photo_remove_dupes(
  '/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/')


photos_dry_run3 <- fpr::fpr_photo_remove_dupes(
  '/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/',
                                               min_replicates = 3)



# actually run the removal of the first un-renamed photo
# fpr::fpr_photo_remove_dupes('/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/',
#                             dry_run = F)

photos_dry_run_after <- fpr::fpr_photo_remove_dupes('/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/')
photos_dry_run3_after <- fpr::fpr_photo_remove_dupes('/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/repo/fish_passage_skeena_2023_reporting/data/photos/sorted/',
                                                     min_replicates = 3)

save(photos_dry_run, photos_dry_run3, photos_dry_run_after, photos_dry_run3_after, file = "data/inputs_extracted/photos_dry_run.RData")

```

# Why Handbombing Photo Resizing can be a Bad Idea

See below - what a nightmare. needed to replace. pretty sure this was the result of point and click windows crap to resize this photo that was added after the fact - again see <https://github.com/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/55>

```{r}
# need to resize the photo that was not resized as per https://github.com/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/55
error_unrezised_perhaps <- magick::image_read("/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/2023_data/skeena/photos/8801379/TimePhoto_20230926_110112_downstream.jpg")
magick::image_info(error_unrezised_perhaps)

# resize in place
fpr::fpr_photo_resize_convert(photo = "/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/2023_data/skeena/photos/8801379/TimePhoto_20230926_110112_downstream.jpg",
                         path = "/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/2023_data/skeena/photos/8801379/")

resized_image <- magick::image_read("/Users/airvine/Library/CloudStorage/OneDrive-Personal/Projects/2023_data/skeena/photos/8801379/TimePhoto_20230926_110112_downstream.jpg")
magick::image_info(resized_image)


# same error... lets look at an amalgamated photo for clues
fpr_photo_amalg_cv(site_id = 8801379,
                   dir_photos = "~/Library/CloudStorage/OneDrive-Personal/Projects/2023_data/skeena/photos/")
```

# QA renamed photos

We have two options for this:

Check for missing photos/duplicates with our imported gpkg built dataframe. As we can see with the result we have no missing photos or duplicates.

```{r qa3, eval = TRUE}
fpr::fpr_photo_qa_df(dat = form_pscis, dir_photos = dir_photos_shared)
```

or

<br>

Check for missing photos/duplicates with the spreadsheets as the input (default)

```         
fpr::fpr_photo_qa_df(dir_photos = dir_photos_shared)
```

** BELOW HAPPENED BEFORE BUT NOW WE KNOW DIFFERENT** 
Although we have a workaround (see above) `fpr::fpr_photo_qa_df` will not run in sub directory `.Rmd` files b/c it contains a helper function (within the `fpr::fpr_import_pscis_all` function) called `fpr_pscis_wkb_paths` and that function has the `path` to our `data` directory hard coded in. Because our `.Rmd` lives in an environment of its own directory (so to speak) we **perhaps** could remidiy the situation by altering `fpr_pscis_wkb_paths` in `fpr` by allowing the `path` argument to be passed to the function, making `path` default to `data` (for backwards compatability) and passing the `...` placeholder from `fpr::fpr_photo_qa_df` to `fpr::fpr_import_pscis_all`. Could be a good idea to do this in the future if we want to continue moving to sub-directory `.Rmd` files for processing data - but will not tackle that now. We will however file an issue so that we can track this.

```{r issue01}
output_issue_create <- gh::gh(
  endpoint = "POST /repos/{owner}/{repo}/issues",
  title = "`fpr_pscis_wkb_paths` should accept a `path` argument",
  body = "`fpr::fpr_photo_qa_df` will not run in sub directory `.Rmd` files b/c it contains a helper function (within the
`fpr::fpr_import_pscis_all` function) called `fpr_pscis_wkb_paths` and that function has the `path` to our `data`
directory hard coded in.  Because our `.Rmd` lives in an environment of its own directory (so to speak) we **perhaps**
could remidiy the situation by altering `fpr_pscis_wkb_paths` in `fpr` by allowing the `path` argument to be passed to the function, making `path` default to `data` (for backwards compatability) and passing the `...` placeholder from `fpr::fpr_photo_qa_df` to 
`fpr::fpr_import_pscis_all`.  Could be a good idea to do this in the future if we want to continue moving to sub-directory
`.Rmd` files for processing data",
  owner = params$repo_owner,
  repo = "fpr"
)
```

<br>

Next we will make sure the site_id's in the two locations are the same. If there not use `waldo::compare` to sleuth it out

```{r qa4}
identical(
  sort(form_pscis$site_id),

  sort(fpr::fpr_import_pscis_all(backup = FALSE) |>
         dplyr::bind_rows() |>
         dplyr::distinct(site_id) |>
         dplyr::pull(site_id))
  )

```

Hmm. We get an error b/c of the `fpr::fpr_import_pscis_all` function. That was enough for us to go back and try to fix issue #78 that we just made in `fpr`. We will now run the above chunk again to see if it works.

```{r qa5}
identical(
  sort(form_pscis$site_id),

  sort(fpr::fpr_import_pscis_all(
    backup = FALSE) |>
         dplyr::bind_rows() |>
         dplyr::distinct(site_id) |>
         dplyr::pull(site_id))
  )

```

Well - it didn't - until we found out about #6 above!

We though that we needed to file an issue to try to address the fact that `fpr_import_pscis` assumes it is operating in the main project directory.\
So we filed this issue.

```{r issue02}
output_issue_create <- gh::gh(
  endpoint = "POST /repos/{owner}/{repo}/issues",
  title = "Consider modifying `fpr_import_pscis` so it no longer needs to operate from the main project directory",
  body = "`fpr::fpr_photo_qa_df` will not run in sub directory `.Rmd` files b/c `fpr_import_pscis` reads
  from a hard coded `data` directory.  We attemped to make this a full path by includkng `getwd()` within `fpr_import_pscis`
  but that does not remedy the issue as then `getwd` spells out the `.Rmd` environment path.  I believe there are `rmarkdown::render`
  params that allow us to switch `environments` but this is too involved for now and we are running in chunks anyway.  
  Fileing this issue now that we are closing #78.",
  owner = params$repo_owner,
  repo = "fpr"
)
```

Comment on the first issue regarding `fpr_pscis_wkb_paths` (was issue #78) and reference the newly created issue. We will close the first issue with the commit message in`fpr`.

```{r issue03}
gh::gh(endpoint = "POST /repos/{owner}/{repo}/issues/78/comments",
       body = paste0(
         "New param `path_ls_files` defaults to `data` (quoted) to allow backwards compatability. See related
       issue #",
       output_issue_create$number
       ),
  owner = params$repo_owner,
  repo = "fpr"
)

```

Now that we are wiser and realize we don't need to try to alter `fpr_import_pscis` we are going to close that issue after we explain why.

```{r}
gh::gh(endpoint = "POST /repos/{owner}/{repo}/issues/79/comments",
       body = "We don't need to try to alter `fpr_import_pscis` for now as we can simply set our `.Rmd` working directory
       options in the `setup` chunk or in our `Rstudio` global options.  We did it globally.",
       owner = params$repo_owner,
       repo = "fpr"
)
```

Now we close it.

```{r}
gh::gh(endpoint = "POST /repos/{owner}/{repo}/issues/79",
       state = "closed",
       owner = params$repo_owner,
       repo = "fpr"
)
```

** ABOVE HAPPENED BEFORE BUT NOW WE KNOW DIFFERENT**

We need to make yet another issue. We really need to develop simple to tests to ensure that when we change our import functions we don't break our imports somehow. This was tested manually during this process by running the imports scripts in the `scripts/tables.R` files but they should be in package using `testthat` as this is a critical step that will break all old reports if something goes sideways. Interestingly (and awesomely) github copilot suggested the function, title and most of the body for this issue below perfectly.

```{r issue04}
output_issue_create <- gh::gh(
  endpoint = "POST /repos/{owner}/{repo}/issues",
  title = "Develop tests for `fpr_import_pscis` to ensure backwards compatability",
  body = "We need to develop tests for `fpr_import_pscis` to ensure that when we change our import functions we don't break our imports somehow.
  This is critical as changes to import functions and helpers can break all old reports if something goes sideways. Related to issue #78 and issue #79",
  owner = params$repo_owner,
  repo = "fpr"
)
```

# Build photo amalgamation for each site

```{r amalg}
# get a list of sites to burn
sites_l <- fpr::fpr_import_pscis_all() %>%
  bind_rows() %>%
  distinct(site_id) %>%
  arrange(site_id) %>%
  pull(site_id)

# burn the amal photos to onedrive
sites_l %>%
  purrr::map(fpr::fpr_photo_amalg_cv, dir_photos = dir_photos_shared)
```

# Find sites that have directories but do not have an entry in the PSCIS spreadsheets

Due to the issues related to relative paths identified and documented above we need to run a workaround to complete our qa. The work around involves running the build of the `pcsis_all` object by running the following line in the console.

```{r qa6}

pscis_all <- fpr::fpr_import_pscis_all() |> dplyr::bind_rows()

print("directories in shared storage location but have no site IDs in pscis input spreadsheets:")
setdiff(
  list.dirs(dir_photos_shared, full.names = F, recursive = F),

  pscis_all %>%
    distinct(site_id) %>%
    arrange(site_id) %>%
    # head() %>% #test
    pull(site_id)
)

print("Have site IDs in pscis input spreadsheets but not in shared storage location:")
setdiff(
    pscis_all %>%
    distinct(site_id) %>%
    arrange(site_id) %>%
    # head() %>% #test
    pull(site_id),
    
    list.dirs(dir_photos_shared, full.names = F, recursive = F)
)
```

result is hand bombed below to track result over time in case something were to change in our input sheets

```         
[1] "directories in shared storage location but not in pscis input spreadsheets "
[1] "123379" "197360" "197378" "197379" "197912" "198060"
[1] "directories in pscis input spreadsheets but not in shared storage location"
numeric(0)
```

So what are these? We need to go to `QGIS` now and have a look at the `bcfishpass.crossings_vw` and/or the `whse_fish.pscis_assessment_svw` layer to understand.

<br>

After review we can see that we have a mix of reasons for this phenomena.

1.  Electrofishing site `123377_ds_ef3` was incorrectly entered in our field form as `123379_ds_ef3`.

How do we know?

<br>

Well - many of these directories were made using `fpr_photo_rename`. This builds the directories based on the name of the site input into our digital fieldforms!

```{r ss01, eval = TRUE}
knitr::include_graphics("fig/Screen Shot 2024-04-26 at 12.26.08 PM.png")
```

-   This makes a bit of sense on review of the attribute table of `form_fiss_site_2023` since this was the only site entered by `newgraph_airvine` . He must have been out to lunch that day 😏.

-   Is this information in an issue somewhere in the repo - no. Not yet. Doesn't make sense why not as the site name must have been changed manually in the QGIS gpkg... We should have made an issue to track this. Now we see why.

-   Is this information preserved in `{QGIS}/form_fiss_site.gpkg` (our form in its original state from the field? No. This is perhaps because this change was made before we updated our workflows to move and rename the original file to `{QGIS}/data_field/2023/form_fiss_site_2023.gpkg`

<br>

How do we remedy?

-   Let's make an issue to track what is happening. let's evolve and use the `gh` package to do this.

```{r}
gh::gh(
  endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues",
  title = "Move Thompson 123379 photos to 123377",
  body = "site_id 123379_ef_ds3 was recorded wrong and was actually 123377_ef_ds3. This was entered incorrectly in the field form. The photos are stored in the wrong directory. We need to move the photos from the 123379 directory to the 123377 directory and then remove the 123379 directory. This is a one time only operation."
)

# sick - let's assign this and change the title
gh::gh(
  endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/69",
  title = "Move misplaced photos from Thompson 123379 to 123377 on onedrive",
  assignee = "NewGraphEnvironment"
)

# lets add labels
gh::gh(
  endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/69/labels",
  labels = c("data managment", "photos")
)

```

-   Let's copy over the named photos from our `123379` directory in our shared location (onedrive) to the correct `123377` directory and remove the `123379` directory.

```{r}
# move the photos
files_from <- list.files(paste0(dir_photos_shared, "123379/"), full.names = T)
files_to <- paste0(dir_photos_shared, "123377/", basename(files_from))

file.copy(from = files_from,
          to = files_to,
          recursive = T)
```

Now - after confirming that went well - we delete the old directory

```{r ss2, eval = TRUE}
knitr::include_graphics("fig/Screen Shot 2024-04-26 at 3.25.06 PM.png")
```

```{r}
# We will use fs in the future!!
unlink(paste0(dir_photos_shared, "123379/"), recursive = TRUE)
```

Ok great. Let's add a comment and close the issue. Two seperate steps:

```{r}
gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/69/comments",
       state = "closed",
       labels = "invalid",
       body = "got er done!")
```

```{r}
gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/69",
       state = "closed")
```

```{r}
knitr::include_graphics("fig/Screen Shot 2024-04-29 at 1.08.59 PM.png")

```

2.  "197360" "197378" "197379" "197912" are sites where electrofishing was conducted as part of baseline monitoring and follow up where habitat confirmations have already been completed but we did not actually do reassessments of the culverts. We often do reassessments of the culverts because:

-   we are there anyway so why not do as good a job as possible

-   people often need practice getting familiar with what we are doing

-   new data can sometimes be better quality than past (ex. sometimes past photos were not taken or taken poorly)

-   It give us insight into the variation about how different surveyors measurements can vary and how even the same surveyor can get different measurements and estimates from the same site when visited multiple times

-   Sometimes there can be changes in the actual values of what we are measuring. For example, very undersized pipes will downcut their outlet pools overtime so the `outlet drop` measurement can actually change

With all that said, we are often extremely strapped for time covering huge areas sometimes all in one day so we do not always reassess the crossings. This is what happened at these crossings.

3.  What about "198060"? This is a strange one. We have photos from the site but we do not have any data.  We may have stopped
at the site to look at fish sampling but did not actually assess the culvert because the stream was completely dry.  Reviewed
all the paper cards and there is no data in case it was done by hand but not in the fieldform. We sampled three differenct streams
this day so it was likely that we just pinned it out of there to get to Reddick and get that done before too long of a day hit
us.

# Make Phase 2 photo directories

For this step we copy phase 1 directories that have phase 2 events and give then the PSCIS ID. We need to do this to complete the reporting.

<br>

First lets get a list of the sites that need IDs updated to Phase 2. We will use the `fpr::fpr_import_pscis` function and cross reference with the `xref_pscis_my_crossing_modelled` object to get the new IDs.

```{r p2-import}
##use the pscis spreadsheet to make the folders to copy the photos to (d is dat)
# we need to mess with the dir_root because we are in a Rmd file now...
d <- fpr::fpr_import_pscis(workbook_name = 'pscis_phase2.xlsm')

# build our xref to get pscis ids - https://github.com/NewGraphEnvironment/fpr/issues/73
# note that we don't actually need to filter this at all since its a join but it is waaay faster and we don't need all 18k records so we will
xref_pscis_my_crossing_modelled <- rfp::rfp_bcd_get_data(
  bcdata_record_id = "WHSE_FISH.PSCIS_ASSESSMENT_SVW",
  col_filter = 'FUNDING_PROJECT_NUMBER',
  # this part is project specific!
  col_filter_value = "skeena_2023_Phase1",
  col_extract = c('EXTERNAL_CROSSING_REFERENCE', 'STREAM_CROSSING_ID'),
  drop_geom = TRUE
)

```

Which are the `external_crossing_reference` (a.k.a `my_crossing_reference`) IDs that we need and which are the newly assigned PSCIS IDs?

```{r p2-id1}
ids_needed <- d %>%
  dplyr::filter(!is.na(my_crossing_reference)) %>%
  dplyr::select(my_crossing_reference) %>%
  dplyr::distinct() %>%
  dplyr::arrange(my_crossing_reference) |> 
  dplyr::pull(my_crossing_reference)

r <- xref_pscis_my_crossing_modelled |> 
  dplyr::filter(external_crossing_reference %in% ids_needed) 

r
```

<br>

Since this workflow is one time only we will print the result here so that we can see what happened in the future even though the `data/pscis_phase2.xlsm` file will then be changed resuling in a zero length tibble from the chunk above .

```{r p2-id2, eval = TRUE}
# r = result for this case
r <- tibble::tribble(
       ~external_crossing_reference, ~stream_crossing_id,
                          14000571L,             198934L,
                          14001106L,             198942L,
                          24601280L,             198947L
       )

r
```

<br>

We could obviously do the updating of the spreadsheet with a join and a csv to get a copy paste file ready but we will change in the PSCIS input sheets by hand. We may want to add this info to the QGIS project `.gpkg` as well but we will wait on that as we should move on. A note on that however - this may be a bit complicated with our `site_id` which should be either the `my_crossing_reference` or the `pscis_crossing_id` - and not both. We will need to think about that a bit more and see how it plays out.

<br>

After updating our `data/pscis_phase2.xlsm` spreadsheet we want to back up the changes and save to git with informative message... To do that we will use the `fpr::import_pscis_all` function which by default (ie. `backup=TRUE`) backs up each `.xlsm` sheet as a seperate `.csv` so `git` can see the changes (excel files are binary like `sqlite` so `git` can't tell and communicate what the changes are - even though `git` knows something has changed and tracks the versions).

```         
?fpr_sheet_trim
```

```{r p2-backup}
fpr::fpr_import_pscis_all()
```

```{r dir-create3}

pscis_new_sites <- dplyr::left_join(
  d,
  xref_pscis_my_crossing_modelled,
  by = c('pscis_crossing_id' = 'stream_crossing_id')
) %>%
  dplyr::filter(!is.na(external_crossing_reference)) |> 
  dplyr::select(external_crossing_reference, pscis_crossing_id)

# seems like the package `fs` may simplify our processes for copying directories. We will try it out here.
fs::dir_copy(
  path = paste0(dir_photos_shared, 
                pscis_new_sites |> pull(external_crossing_reference) |> as.character()),
  new_path = paste0(dir_photos_shared,
                    pscis_new_sites |> pull(pscis_crossing_id) |> as.character())
)
```

WOW! `fs` is a massive improvement over our past hard coded directory copy systems. We will watch out for how to 
simplify lots of those workflows as there are many!

<br>

Let's close the issue we opened about this needing to happen.

```{r}

gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/67/comments",
       body = "This is complete on onedrive as per documentation in `scripts/01_prep_inputs/01_prep_photos.Rmd` and
       `scripts/01_prep_inputs/01_prep_photos.html`.")

gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/67",
       state = "closed")

```

# Copy Over all Photos to Local Repo

Here is our next simplification of workflows due to the new to us `fs` package. We will copy all of the photos from our 
onedrive to our local repo.  We will use the `fs::dir_copy` function to do this.  I made an issue by hand that we are 
about to do this.

<br>

**Note** - we are going to nuke our folder first (why not?!) then overwrite the `photo_sort_tracking_phase1.csv` file that is in the `data` directory. We will
need to store these tracking files elsewhere in the future and perhaps `backup` is a good spot to do that so I moved it there
for now.


```{r}
fs::dir_delete("data/photos")

fs::dir_copy(
  path = dir_photos_shared,
  new_path = "data/",
  overwrite = TRUE)

```

Amazing. So easy. Let's close the issue we opened about this needing to happen.

```{r}

gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/70/comments",
       body = "This is complete locally (ie. Al's repo) as per documentation in `scripts/01_prep_inputs/01_prep_photos.Rmd` and
       `scripts/01_prep_inputs/01_prep_photos.html`.  **Important** - we are nuking (deleting) the entire directory
       every time with `fs::dir_delete('data/photos')` so we need to be sure to only make changes in our shared
       location.  For the record the way to do this is 
       `fs::dir_copy(
  path = dir_photos_shared,
  new_path = 'data/',
  overwrite = TRUE)`")

gh::gh(endpoint = "POST /repos/NewGraphEnvironment/fish_passage_skeena_2023_reporting/issues/70",
       state = "closed")

```


We still cannot knit this with the `knit` button now that the knit directory is set to `Project` directory!  We can 
run the chunks one by one though which is huge.  We copy below to terminal to build the documentation.

```{r render, eval = FALSE}
rmarkdown::render(input = file_in, output_file = file_out)
```
